{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a29780b7-bb0d-456a-af3e-7f16bb3a0800",
   "metadata": {},
   "source": [
    "SinkSAM Data preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a3a2ad-c791-41ba-a8e2-37a8116727d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "def split_dataset(source_folder, train_folder, valid_folder, train_ratio=0.7):\n",
    "    # Ensure the output directories exist\n",
    "    os.makedirs(train_folder, exist_ok=True)\n",
    "    os.makedirs(valid_folder, exist_ok=True)\n",
    "\n",
    "    # Get a list of all TIFF files in the source folder\n",
    "    tiff_files = [f for f in os.listdir(source_folder) if f.endswith('.tiff') or f.endswith('.tif')]\n",
    "\n",
    "    # Shuffle the list to ensure randomness\n",
    "    random.shuffle(tiff_files)\n",
    "\n",
    "    # Calculate the split index\n",
    "    split_index = int(len(tiff_files) * train_ratio)\n",
    "\n",
    "    # Split the files into train and validation sets\n",
    "    train_files = tiff_files[:split_index]\n",
    "    valid_files = tiff_files[split_index:]\n",
    "\n",
    "    # Copy the files to their respective directories\n",
    "    for file in train_files:\n",
    "        shutil.copy(os.path.join(source_folder, file), os.path.join(train_folder, file))\n",
    "\n",
    "    for file in valid_files:\n",
    "        shutil.copy(os.path.join(source_folder, file), os.path.join(valid_folder, file))\n",
    "\n",
    "    print(f\"Total files: {len(tiff_files)}\")\n",
    "    print(f\"Train files: {len(train_files)}\")\n",
    "    print(f\"Validation files: {len(valid_files)}\")\n",
    "\n",
    "# Define source and destination folders\n",
    "source_folder = \"D:/Osher/ann_osher/sinkholes_yolo_dataset/images_6bands/\"\n",
    "train_folder = \"D:/Osher/ann_osher/sinkholes_yolo_dataset/train_6bands/\"\n",
    "valid_folder = \"D:/Osher/ann_osher/sinkholes_yolo_dataset/valid_6bands/\"\n",
    "\n",
    "# Call the function to split the dataset\n",
    "split_dataset(source_folder, train_folder, valid_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4a679a-6a65-4dc8-be30-19e598035746",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os ## new one\n",
    "import numpy as np\n",
    "import tifffile\n",
    "import rasterio\n",
    "\n",
    "# Directory containing the TIFF images\n",
    "input_folder = \"D:/Osher/ann_osher/sinkholes_yolo_dataset/valid_6bands/\"\n",
    "output_folder_rgb = \"D:/Osher/ann_osher/sinkholes_yolo_dataset/input/valid_images\"\n",
    "output_folder_masks1 = \"D:/Osher/ann_osher/sinkholes_yolo_dataset/input/valid_masks/Sinkholes\"\n",
    "\n",
    "# Ensure output directories exist\n",
    "os.makedirs(output_folder_rgb, exist_ok=True)\n",
    "os.makedirs(output_folder_masks1, exist_ok=True)\n",
    "\n",
    "# Function to convert images to 8-bit\n",
    "def convert_to_8bit(image):\n",
    "    return image.astype(np.uint8)\n",
    "\n",
    "# Iterate through each file in the input folder\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith(\".tif\") or filename.endswith(\".tiff\"):\n",
    "        # Open the image\n",
    "        image_path = os.path.join(input_folder, filename)\n",
    "        \n",
    "        try:\n",
    "            # Load the TIFF image using tifffile\n",
    "            image = tifffile.imread(image_path)\n",
    "\n",
    "            # Check if the image has at least 6 bands\n",
    "            if image.shape[-1] >= 6:\n",
    "                # Extract the bands\n",
    "                band_1 = convert_to_8bit(image[..., 0])\n",
    "                band_2 = convert_to_8bit(image[..., 1])\n",
    "                band_3 = convert_to_8bit(image[..., 2])\n",
    "                band_5 = convert_to_8bit(image[..., 4])\n",
    "                band_6 = convert_to_8bit(image[..., 5])\n",
    "\n",
    "                # Check if mask1 (band 5) has any values greater than 1 or all values are zero\n",
    "                if np.any(band_5 > 1) or np.all(band_5 == 0):\n",
    "                    print(f\"Mask1 (band 5) in file {image_path} contains values greater than 1 or all values are zero. Skipping RGB and mask output.\")\n",
    "                    continue\n",
    "\n",
    "                # Create the RGB image using bands 1, 2, and 3\n",
    "                rgb_image = np.stack([band_1, band_2, band_3], axis=-1)\n",
    "\n",
    "                # Save the RGB image\n",
    "                output_rgb_path = os.path.join(output_folder_rgb, filename)\n",
    "                with rasterio.open(output_rgb_path, 'w', driver='GTiff', height=rgb_image.shape[0], width=rgb_image.shape[1], count=3, dtype='uint8', photometric='RGB') as dst:\n",
    "                    for i in range(3):\n",
    "                        dst.write(rgb_image[:, :, i], i + 1)\n",
    "\n",
    "                # Save the mask1 (band 5)\n",
    "                mask1_image = band_5\n",
    "                output_mask1_path = os.path.join(output_folder_masks1, filename)\n",
    "                with rasterio.open(output_mask1_path, 'w', driver='GTiff', height=mask1_image.shape[0], width=mask1_image.shape[1], count=1, dtype='uint8') as dst:\n",
    "                    dst.write(mask1_image, 1)\n",
    "\n",
    "            else:\n",
    "                print(f\"Image file {image_path} does not have at least 6 bands.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image file {image_path}: {e}\")\n",
    "\n",
    "print(\"Processing completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb50360-fd5a-4b08-a10c-744145e5fd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os ##convert labels to 8 bit\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from rasterio.enums import Resampling\n",
    "\n",
    "def process_tiff_folder(input_folder, output_folder):\n",
    "    \"\"\"\n",
    "    Process all TIFF files in the input folder, convert all non-zero values to 1, and save them in 8-bit format.\n",
    "    \n",
    "    :param input_folder: Path to the folder containing the input TIFF files\n",
    "    :param output_folder: Path to the folder to save the processed TIFF files\n",
    "    \"\"\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.endswith(\".tif\") or filename.endswith(\".tiff\"):\n",
    "            input_path = os.path.join(input_folder, filename)\n",
    "            output_path = os.path.join(output_folder, filename)\n",
    "            \n",
    "            with rasterio.open(input_path) as src:\n",
    "                # Read the image data\n",
    "                image = src.read(1)\n",
    "                transform = src.transform\n",
    "                crs = src.crs\n",
    "            \n",
    "            # Convert all non-zero values to 1\n",
    "            processed_image = np.where(image != 0, 1, 0).astype(np.uint8)\n",
    "            \n",
    "            # Save the processed image\n",
    "            with rasterio.open(\n",
    "                output_path,\n",
    "                'w',\n",
    "                driver='GTiff',\n",
    "                height=processed_image.shape[0],\n",
    "                width=processed_image.shape[1],\n",
    "                count=1,\n",
    "                dtype='uint8',\n",
    "                crs=crs,\n",
    "                transform=transform\n",
    "            ) as dst:\n",
    "                dst.write(processed_image, 1)\n",
    "            \n",
    "            print(f\"Processed {filename} and saved to {output_path}\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "input_folder = \"D:/Osher/ann_osher/YOLO_DATABASE/input/val_masks/Sinkholes/\"\n",
    "output_folder = \"D:/Osher/ann_osher/YOLO_DATABASE/input/val_masks/Sinkholes/\"\n",
    "\n",
    "process_tiff_folder(input_folder, output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d26f48-30e1-4b74-abd8-592fe733c32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "# Label IDs of the dataset representing different categories\n",
    "category_ids = {\n",
    "    \"Sinkholes\": 1\n",
    "}\n",
    "\n",
    "MASK_EXT = 'tif'\n",
    "ORIGINAL_EXT = 'tif'\n",
    "image_id = 0\n",
    "annotation_id = 0\n",
    "\n",
    "def images_annotations_info(maskpath):\n",
    "    \"\"\"\n",
    "    Process the binary masks and generate images and annotations information.\n",
    "\n",
    "    :param maskpath: Path to the directory containing binary masks\n",
    "    :return: Tuple containing images info, annotations info, and annotation count\n",
    "    \"\"\"\n",
    "    global image_id, annotation_id\n",
    "    annotations = []\n",
    "    images = []\n",
    "\n",
    "    # Iterate through categories and corresponding masks\n",
    "    for category in category_ids.keys():\n",
    "        for mask_image in glob.glob(os.path.join(maskpath, category, f'*.{MASK_EXT}')):\n",
    "            original_file_name = f'{os.path.basename(mask_image).split(\".\")[0]}.{ORIGINAL_EXT}'\n",
    "            mask_image_open = cv2.imread(mask_image)\n",
    "            \n",
    "            # Check if the image was loaded successfully\n",
    "            if mask_image_open is None:\n",
    "                print(f\"Error loading image: {mask_image}\")\n",
    "                continue\n",
    "            \n",
    "            # Get image dimensions\n",
    "            height, width, _ = mask_image_open.shape\n",
    "\n",
    "            # Create or find existing image annotation\n",
    "            if original_file_name not in map(lambda img: img['file_name'], images):\n",
    "                image = {\n",
    "                    \"id\": image_id + 1,\n",
    "                    \"width\": width,\n",
    "                    \"height\": height,\n",
    "                    \"file_name\": original_file_name,\n",
    "                }\n",
    "                images.append(image)\n",
    "                image_id += 1\n",
    "            else:\n",
    "                image = [element for element in images if element['file_name'] == original_file_name][0]\n",
    "\n",
    "            # Find contours in the mask image\n",
    "            gray = cv2.cvtColor(mask_image_open, cv2.COLOR_BGR2GRAY)\n",
    "            _, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "            contours = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)[0]\n",
    "\n",
    "            # Create annotation for each contour\n",
    "            for contour in contours:\n",
    "                bbox = cv2.boundingRect(contour)\n",
    "                area = cv2.contourArea(contour)\n",
    "                segmentation = contour.flatten().tolist()\n",
    "\n",
    "                annotation = {\n",
    "                    \"iscrowd\": 0,\n",
    "                    \"id\": annotation_id,\n",
    "                    \"image_id\": image['id'],\n",
    "                    \"category_id\": category_ids[category],\n",
    "                    \"bbox\": bbox,\n",
    "                    \"area\": area,\n",
    "                    \"segmentation\": [segmentation],\n",
    "                }\n",
    "\n",
    "                # Add annotation if area is greater than zero and not a duplicate\n",
    "                if area > 0 and annotation not in annotations:\n",
    "                    annotations.append(annotation)\n",
    "                    annotation_id += 1\n",
    "\n",
    "    return images, annotations, annotation_id\n",
    "\n",
    "\n",
    "def process_masks(mask_path, dest_json):\n",
    "    global image_id, annotation_id\n",
    "    image_id = 0\n",
    "    annotation_id = 0\n",
    "\n",
    "    # Initialize the COCO JSON format with categories\n",
    "    coco_format = {\n",
    "        \"info\": {},\n",
    "        \"licenses\": [],\n",
    "        \"images\": [],\n",
    "        \"categories\": [{\"id\": value, \"name\": key, \"supercategory\": key} for key, value in category_ids.items()],\n",
    "        \"annotations\": [],\n",
    "    }\n",
    "\n",
    "    # Create images and annotations sections\n",
    "    coco_format[\"images\"], coco_format[\"annotations\"], annotation_cnt = images_annotations_info(mask_path)\n",
    "\n",
    "    # Save the COCO JSON to a file\n",
    "    with open(dest_json, \"w\") as outfile:\n",
    "        json.dump(coco_format, outfile, sort_keys=True, indent=4)\n",
    "\n",
    "    print(\"Created %d annotations for images in folder: %s\" % (annotation_cnt, mask_path))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_mask_path = \"D:/Osher/ann_osher/sinkholes_yolo_dataset/input/train_masks/\"\n",
    "    train_json_path = \"D:/Osher/ann_osher/sinkholes_yolo_dataset/input/train_images/train.json\"\n",
    "    process_masks(train_mask_path, train_json_path)\n",
    "\n",
    "    val_mask_path = \"D:/Osher/ann_osher/sinkholes_yolo_dataset/input/val_masks/\"\n",
    "    val_json_path = \"D:/Osher/ann_osher/sinkholes_yolo_dataset/input/val_images/val.json\"\n",
    "    process_masks(val_mask_path, val_json_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f1a24d-f133-4aaf-9d74-4adeb584dd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import yaml\n",
    "\n",
    "# Function to convert images to YOLO format\n",
    "def convert_to_yolo(input_images_path, input_json_path, output_images_path, output_labels_path):\n",
    "    # Open JSON file containing image annotations\n",
    "    f = open(input_json_path)\n",
    "    data = json.load(f)\n",
    "    f.close()\n",
    "\n",
    "    # Create directories for output images and labels\n",
    "    os.makedirs(output_images_path, exist_ok=True)\n",
    "    os.makedirs(output_labels_path, exist_ok=True)\n",
    "\n",
    "    # List to store filenames\n",
    "    file_names = []\n",
    "    for filename in os.listdir(input_images_path):\n",
    "        if filename.endswith(\".tif\"):\n",
    "            source = os.path.join(input_images_path, filename)\n",
    "            destination = os.path.join(output_images_path, filename)\n",
    "            shutil.copy(source, destination)\n",
    "            file_names.append(filename)\n",
    "\n",
    "    # Function to get image annotations\n",
    "    def get_img_ann(image_id):\n",
    "        return [ann for ann in data['annotations'] if ann['image_id'] == image_id]\n",
    "\n",
    "    # Function to get image data\n",
    "    def get_img(filename):\n",
    "        return next((img for img in data['images'] if img['file_name'] == filename), None)\n",
    "\n",
    "    # Iterate through filenames and process each image\n",
    "    for filename in file_names:\n",
    "        img = get_img(filename)\n",
    "        img_id = img['id']\n",
    "        img_w = img['width']\n",
    "        img_h = img['height']\n",
    "        img_ann = get_img_ann(img_id)\n",
    "\n",
    "        # Write normalized polygon data to a text file\n",
    "        if img_ann:\n",
    "            with open(os.path.join(output_labels_path, f\"{os.path.splitext(filename)[0]}.txt\"), \"a\") as file_object:\n",
    "                for ann in img_ann:\n",
    "                    current_category = ann['category_id'] - 1\n",
    "                    polygon = ann['segmentation'][0]\n",
    "                    normalized_polygon = [format(coord / img_w if i % 2 == 0 else coord / img_h, '.6f') for i, coord in enumerate(polygon)]\n",
    "                    file_object.write(f\"{current_category} \" + \" \".join(normalized_polygon) + \"\\n\")\n",
    "\n",
    "# Function to create a YAML file for the dataset\n",
    "def create_yaml(input_json_path, output_yaml_path, train_path, val_path, test_path=None):\n",
    "    with open(input_json_path) as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Extract the category names\n",
    "    names = [category['name'] for category in data['categories']]\n",
    "    \n",
    "    # Number of classes\n",
    "    nc = len(names)\n",
    "\n",
    "    # Create a dictionary with the required content\n",
    "    yaml_data = {\n",
    "        'names': names,\n",
    "        'nc': nc,\n",
    "        'test': test_path if test_path else '',\n",
    "        'train': train_path,\n",
    "        'val': val_path\n",
    "    }\n",
    "\n",
    "    # Write the dictionary to a YAML file\n",
    "    with open(output_yaml_path, 'w') as file:\n",
    "        yaml.dump(yaml_data, file, default_flow_style=False)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    base_input_path = \"D:/Osher/ann_osher/sinkholes_yolo_dataset/input/\"\n",
    "    base_output_path = \"D:/Osher/ann_osher/sinkholes_yolo_dataset/yolo_dataset/\"\n",
    "\n",
    "    # Processing validation dataset (if needed)\n",
    "    convert_to_yolo(\n",
    "        input_images_path=os.path.join(base_input_path, \"val_images\"),\n",
    "        input_json_path=os.path.join(base_input_path, \"val_images/val.json\"),\n",
    "        output_images_path=os.path.join(base_output_path, \"valid/images\"),\n",
    "        output_labels_path=os.path.join(base_output_path, \"valid/labels\")\n",
    "    )\n",
    "\n",
    "    # Processing training dataset \n",
    "    convert_to_yolo(\n",
    "        input_images_path=os.path.join(base_input_path, \"train_images\"),\n",
    "        input_json_path=os.path.join(base_input_path, \"train_images/train.json\"),\n",
    "        output_images_path=os.path.join(base_output_path, \"train/images\"),\n",
    "        output_labels_path=os.path.join(base_output_path, \"train/labels\")\n",
    "    )\n",
    "    \n",
    "    # Creating the YAML configuration file\n",
    "    create_yaml(\n",
    "        input_json_path=os.path.join(base_input_path, \"train_images/train.json\"),\n",
    "        output_yaml_path=os.path.join(base_output_path, \"data.yaml\"),\n",
    "        train_path=\"D:/Osher/ann_osher/sinkholes_yolo_dataset/yolo_dataset/train/images/\",\n",
    "        val_path=\"D:/Osher/ann_osher/sinkholes_yolo_dataset/yolo_dataset/valid/images/\",\n",
    "        test_path='../test/images'  # or None if not applicable\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e0eb03-669d-4c04-b5d2-a15a12d2f712",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os ## genetate bbox from computed closed depressions\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from scipy.ndimage import label\n",
    "\n",
    "def mask_to_bboxes(mask, class_id=0, confidence=1.0):\n",
    "    \"\"\"\n",
    "    Convert a mask to bounding boxes for each connected component.\n",
    "    \n",
    "    :param mask: Numpy array of the mask\n",
    "    :param class_id: Class ID to assign to each bounding box\n",
    "    :param confidence: Confidence score to assign to each bounding box\n",
    "    :return: List of bounding boxes in the format [class_id, x_min, y_min, x_max, y_max, confidence]\n",
    "    \"\"\"\n",
    "    labeled_mask, num_features = label(mask)\n",
    "    bboxes = []\n",
    "    for i in range(1, num_features + 1):\n",
    "        coords = np.argwhere(labeled_mask == i)\n",
    "        if len(coords) > 0:\n",
    "            y_min, x_min = coords.min(axis=0)\n",
    "            y_max, x_max = coords.max(axis=0)\n",
    "            bboxes.append([class_id, x_min, y_min, x_max, y_max, confidence])\n",
    "    return bboxes\n",
    "\n",
    "def save_bboxes_to_txt(bboxes, file_path):\n",
    "    \"\"\"\n",
    "    Save bounding boxes to a text file.\n",
    "    \n",
    "    :param bboxes: List of bounding boxes in the format [class_id, x_min, y_min, x_max, y_max, confidence]\n",
    "    :param file_path: Path to the text file\n",
    "    \"\"\"\n",
    "    with open(file_path, 'w') as file:\n",
    "        for bbox in bboxes:\n",
    "            file.write(f\"{bbox[0]} {bbox[1]} {bbox[2]} {bbox[3]} {bbox[4]} {bbox[5]}\\n\")\n",
    "\n",
    "def process_masks_and_save_bboxes(mask_folder, output_folder):\n",
    "    \"\"\"\n",
    "    Process masks and save bounding boxes to text files.\n",
    "    \n",
    "    :param mask_folder: Path to the folder containing masks\n",
    "    :param output_folder: Path to the folder to save the bounding boxes files\n",
    "    \"\"\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    for filename in os.listdir(mask_folder):\n",
    "        if filename.endswith(\".tif\") or filename.endswith(\".tiff\"):\n",
    "            mask_path = os.path.join(mask_folder, filename)\n",
    "\n",
    "            # Read the mask\n",
    "            with rasterio.open(mask_path) as src:\n",
    "                mask = src.read(1)\n",
    "\n",
    "            # Convert mask to bounding boxes\n",
    "            bboxes = mask_to_bboxes(mask)\n",
    "\n",
    "            # Ensure at least one bbox is created\n",
    "            if not bboxes:\n",
    "                # If no bboxes, create a bbox around the center of the mask as a fallback\n",
    "                center_y, center_x = mask.shape[0] // 2, mask.shape[1] // 2\n",
    "                bboxes = [[0, center_x-1, center_y-1, center_x+1, center_y+1, 1.0]]\n",
    "\n",
    "            # Save original bounding boxes to text file\n",
    "            output_txt_path = os.path.join(output_folder, filename.replace(\".tif\", \".txt\").replace(\".tiff\", \".txt\"))\n",
    "            save_bboxes_to_txt(bboxes, output_txt_path)\n",
    "\n",
    "            print(f\"Processed {filename} and saved bounding boxes to {output_txt_path}\")\n",
    "\n",
    "# Define the folder paths\n",
    "mask_folder = \"D:/Osher/ann_osher/sinkholes_yolo_dataset/final_test_512_new/images2/\"\n",
    "output_folder = \"D:/Osher/ann_osher/sinkholes_yolo_dataset/final_test_512_new/images2/boxes_from_masks\"\n",
    "\n",
    "# Process the masks and save bounding boxes\n",
    "process_masks_and_save_bboxes(mask_folder, output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c3b79d-ab2d-4a73-9b19-5eca75e98468",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
